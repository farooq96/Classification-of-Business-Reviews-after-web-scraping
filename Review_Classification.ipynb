{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## COMP41680 Assignment 2: Text Classification  \n",
    " \n",
    " Submitted By : Farooq Shaikh\n",
    " \n",
    " Student Number: 19200161"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description:\n",
    "The assignment involves the scraping of the consumer reviews from a webpage anf trainig a model to classify the reviews as either Positive or Negative. \n",
    "The aasignment has been divided in following three sections:\n",
    "\n",
    "<font color='red'>Section 1:</font> Scraping of the reviews for three categories of the business, namely Automobile, Fashion and Cafe. Once Extracted we Text preprocessing on it and store the clean reiview text in a CSV for future use.\n",
    "\n",
    "<font color='red'>Section 2:</font> This section involves building and testing of classification model on each of the category of the review. We build three different models employing three classification algorithm; Random forest, Naive Bayes, Logstic Regression. Each of the model is tested on each category to find the best performing model for each category.\n",
    "\n",
    "<font color='red'>Section 3:</font> The final section involves cross category testing of the models. We use the trained model from one category say, Automobile and then test it on the other two categories to see if the models are able to genralize across the categories. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "\n",
    "import requests\n",
    "import bs4\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "from IPython.display import HTML\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the address of the webpage \n",
    "web_url= 'http://mlg.ucd.ie/modules/yalp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the webpage html\n",
    "def get_html_parsed(url):\n",
    "    result  = requests.get(url)\n",
    "    parsed_result = bs4.BeautifulSoup(result.text)\n",
    "    return parsed_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "<meta content=\"noindex\" name=\"robots\"/>\n",
       "<meta content=\"Content on this site is posted for teaching purposes only. Original data is from yelp.com\" name=\"description\"/>\n",
       "<meta charset=\"utf-8\"/>\n",
       "<meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/>\n",
       "<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
       "<title>Yalp Home</title>\n",
       "<link href=\"images/favicon.ico\" rel=\"shortcut icon\"/>\n",
       "<!-- Bootstrap core CSS -->\n",
       "<link href=\"assets/css/bootstrap.css\" rel=\"stylesheet\"/>\n",
       "<!-- Custom styles for this template -->\n",
       "<link href=\"assets/css/style.css\" rel=\"stylesheet\"/>\n",
       "<link href=\"assets/css/font-awesome.min.css\" rel=\"stylesheet\"/>\n",
       "<script src=\"assets/js/modernizr.js\"></script>\n",
       "</head>\n",
       "<body>\n",
       "<div class=\"container mtb\">\n",
       "<div class=\"row\">\n",
       "<div class=\"col-md-12\">\n",
       "<h3 class=\"info\"><a class=\"info\" href=\"index.html\">Yalp</a> — Home</h3>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"row\">\n",
       "<div class=\"col-md-10\" id=\"content\">\n",
       "<div id=\"all\">\n",
       "<p>Select a business category from the list below to browse customer reviews:</p>\n",
       "<div class=\"category\"><h4><a href=\"automotive_list.html\">Category: Automotive</a>  (132 businesses)</h4></div>\n",
       "<div class=\"category\"><h4><a href=\"cafes_list.html\">Category: Cafes</a>  (96 businesses)</h4></div>\n",
       "<div class=\"category\"><h4><a href=\"fashion_list.html\">Category: Fashion</a>  (159 businesses)</h4></div>\n",
       "<div class=\"category\"><h4><a href=\"gym_list.html\">Category: Gym</a>  (122 businesses)</h4></div>\n",
       "<div class=\"category\"><h4><a href=\"hair_salons_list.html\">Category: Hair and Salons</a>  (143 businesses)</h4></div>\n",
       "<div class=\"category\"><h4><a href=\"hotels_list.html\">Category: Hotels</a>  (113 businesses)</h4></div>\n",
       "<div class=\"category\"><h4><a href=\"restaurants_list.html\">Category: Restaurants</a>  (100 businesses)</h4></div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"footer\">\n",
       "        Content on this site is posted for teaching purposes only. \n",
       "        </div>\n",
       "</div>\n",
       "<!-- Bootstrap core JavaScript\n",
       "    ================================================== -->\n",
       "<!-- Placed at the end of the document so the pages load faster -->\n",
       "<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js\"></script>\n",
       "<script src=\"assets/js/bootstrap.min.js\"></script>\n",
       "<script src=\"assets/js/retina-1.1.0.js\"></script>\n",
       "<script src=\"assets/js/jquery.hoverdir.js\"></script>\n",
       "<script src=\"assets/js/jquery.hoverex.min.js\"></script>\n",
       "<script src=\"assets/js/jquery.isotope.min.js\"></script>\n",
       "<script src=\"assets/js/custom.js\"></script>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the page to see the tags present for scraping \n",
    "page1=get_html_parsed(web_url)\n",
    "page1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to extract all the links present in the webpage based on the tag\n",
    "def links_extracter(param):\n",
    "    all_links = []\n",
    "    soup_parser = bs4.BeautifulSoup(str(param),\"html.parser\")\n",
    "    for url in soup_parser.findAll('a'):\n",
    "        all_links.append(url.get('href'))\n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all the extracted links in the form of a dataframe\n",
    "links=links_extracter(page1)\n",
    "links.pop(0)\n",
    "links_display=links\n",
    "links_display = [x[:-10] for x in links_display]\n",
    "links_df=pd.DataFrame(links_display)\n",
    "links_df.index.names = ['Link Number']\n",
    "links_df.columns = ['Business Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Business Category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Link Number</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>automotive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cafes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fashion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gym</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hair_salons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hotels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>restaurants</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Business Category\n",
       "Link Number                  \n",
       "0                  automotive\n",
       "1                       cafes\n",
       "2                     fashion\n",
       "3                         gym\n",
       "4                 hair_salons\n",
       "5                      hotels\n",
       "6                 restaurants"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the business from one category \n",
    "def get_business_home_page_urls(link_number):\n",
    "    category_page_part_url = web_url+'/'+links[link_number]\n",
    "    # get links of each individual buisness page url\n",
    "    business_pages_url= links_extracter(get_html_parsed(category_page_part_url))\n",
    "    # combine the link with base URL to get link of each automobile business home page\n",
    "    all_business_pages_urls=[]\n",
    "    for link in business_pages_url :\n",
    "        all_business_pages_urls.append(web_url+'/'+link)\n",
    "    all_business_pages_urls.pop(0) # remove the first index.html one since it is useless\n",
    "    all_business_pages_urls[:5] \n",
    "    return all_business_pages_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select the link number from the above DF\n",
    "automobile_pages_urls = get_business_home_page_urls(0)\n",
    "cafes_pages_urls=get_business_home_page_urls(1)\n",
    "fashion_pages_urls=get_business_home_page_urls(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://mlg.ucd.ie/modules/yalp/review_set_BS4u8wkRb70FDBZcYkJ9kQ.html',\n",
       " 'http://mlg.ucd.ie/modules/yalp/review_set_3O2HLFpUZKWxE55Mpzg81g.html',\n",
       " 'http://mlg.ucd.ie/modules/yalp/review_set_jeG-c5GwD0Uf0Ou5DLFTZg.html',\n",
       " 'http://mlg.ucd.ie/modules/yalp/review_set_UGQEXFibRZIebMRkuJqh3Q.html',\n",
       " 'http://mlg.ucd.ie/modules/yalp/review_set_o1IS7-SvFXmijKDc6UY55Q.html']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automobile_pages_urls[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_rating_extract(category_url):    \n",
    "    business_reviews_df=[] \n",
    "    reviews=[] \n",
    "    ratings=[] \n",
    "    for each_link in category_url:\n",
    "        html = get_html_parsed(each_link)\n",
    "        parser = bs4.BeautifulSoup(str(html),\"html.parser\")\n",
    "        # Text from the review is extracted based on the tag present \n",
    "        for each_review in parser.findAll('p',class_='review-text'):\n",
    "            reviews.append(each_review.text)\n",
    "        # the rating is found from the img and alt tag \n",
    "        for each_rating in parser.findAll('img'):\n",
    "            if each_rating.get('alt')!=None:\n",
    "                # if the stars are less than 4 (0,1,3) then we term it as negative else positive\n",
    "                if int(each_rating.get('alt')[0])<4:\n",
    "                    ratings.append('negative')\n",
    "                else:\n",
    "                    ratings.append('positive')  \n",
    "        business_reviews_df=(pd.DataFrame(reviews)) \n",
    "        business_reviews_df['Rating']=ratings  \n",
    "        business_reviews_df.columns=['Review','Rating']\n",
    "    return business_reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all the reviews from the category URL by passing it to the review_rating _extract function\n",
    "automobile_reviews_df= review_rating_extract(automobile_pages_urls)\n",
    "fashion_reviews_df= review_rating_extract(fashion_pages_urls)\n",
    "cafes_reviews_df= review_rating_extract(cafes_pages_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the output of the extracted reviews and rating for automobile category\n",
    "automobile_reviews_df.to_csv('data/raw/automobile_reviews.csv', index = False)\n",
    "automobile_reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the output of the extracted reviews and rating for fashion category\n",
    "fashion_reviews_df.to_csv('data/raw/fashion_reviews.csv', index = False)\n",
    "fashion_reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the output of the extracted reviews and rating for cafe category\n",
    "cafes_reviews_df.to_csv('data/raw/cafes_reviews.csv', index = False)\n",
    "cafes_reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Raw data and preprocess it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have sucessfully scraped all the reviews for the three bussiness categories. The next step involves cleaning and preprocessing it. we will be applying the standard text precrossing techniques such as tokenization, removal of Non-ASCII characters, remove stopwords and lemmaization. \n",
    "\n",
    "Tokenization breaks the sentences into individual tokens while removing the stopwords and Non-ASCII characters will ensure that the we only have the words that convey some meaning about the nature of the review. The final step is Lemmatization which essentially converts all words to their roots or lemma. For example the words plays, played, playing all are the modified form of the root word 'play' and hence can be reduced to it by the process of lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automobile_reviews_df = pd.read_csv('data/raw/automobile_reviews.csv')\n",
    "fashion_reviews_df = pd.read_csv('data/raw/fashion_reviews.csv')\n",
    "cafes_reviews_df = pd.read_csv('data/raw/cafes_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to preprocess the data \n",
    "\n",
    "# Function to remove non-ascii characters from the text\n",
    "def _removeNonAscii(s): \n",
    "    return \"\".join(i for i in s if ord(i)<128)\n",
    "\n",
    "# function to remove the punctuations, apostrophe, special characters using regular expressions\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = text.replace('(ap)', '')\n",
    "    text = re.sub(r\"\\'s\", \" is \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r\"\\\\\", \"\", text)\n",
    "    text = re.sub(r\"\\'\", \"\", text)    \n",
    "    text = re.sub(r\"\\\"\", \"\", text)\n",
    "    text = re.sub('[^a-zA-Z ?!]+', '', text)\n",
    "    text = _removeNonAscii(text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# stop words are the words that convery little to no information about the actual content like the words:the, of, for etc\n",
    "\n",
    "def remove_stopwords(word_tokens):\n",
    "    filtered_sentence = [] \n",
    "    stop_words = stopwords.words('english')\n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w) \n",
    "    return filtered_sentence\n",
    "\n",
    "# splitting a string, text into a list of tokens\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "def tokenize(x): \n",
    "    return tokenizer.tokenize(x)\n",
    "\n",
    "# lemmatization of the reviews\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "    \n",
    "def detokenize(tokens):\n",
    "    sent = TreebankWordDetokenizer().detokenize(tokens)\n",
    "    return sent \n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying all the preprocessing Functions to all the categories and storing the clean & preprocessed File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# applying all of these functions to the our dataframe \n",
    "automobile_reviews_df['precrocessed_review'] = automobile_reviews_df['Review'].map(clean_text)\n",
    "automobile_reviews_df['precrocessed_review'] = automobile_reviews_df['precrocessed_review'].map(tokenize)\n",
    "automobile_reviews_df['precrocessed_review'] = automobile_reviews_df['precrocessed_review'].map(remove_stopwords)\n",
    "automobile_reviews_df['precrocessed_review'] = automobile_reviews_df['precrocessed_review'].map(detokenize)\n",
    "automobile_reviews_df['precrocessed_review'] = automobile_reviews_df['precrocessed_review'].map(lemmatize_sentence)\n",
    "\n",
    "# checking the clean data \n",
    "automobile_reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying all the functions to the second category of data\n",
    "\n",
    "fashion_reviews_df['precrocessed_review'] = fashion_reviews_df['Review'].map(clean_text)\n",
    "fashion_reviews_df['precrocessed_review'] = fashion_reviews_df['precrocessed_review'].map(tokenize)\n",
    "fashion_reviews_df['precrocessed_review'] = fashion_reviews_df['precrocessed_review'].map(remove_stopwords)\n",
    "fashion_reviews_df['precrocessed_review'] = fashion_reviews_df['precrocessed_review'].map(detokenize)\n",
    "fashion_reviews_df['precrocessed_review'] = fashion_reviews_df['precrocessed_review'].map(lemmatize_sentence)\n",
    "\n",
    "# checking the clean data \n",
    "fashion_reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying all the functions to the third category of data\n",
    "\n",
    "cafes_reviews_df['precrocessed_review'] = cafes_reviews_df['Review'].map(clean_text)\n",
    "cafes_reviews_df['precrocessed_review'] = cafes_reviews_df['precrocessed_review'].map(tokenize)\n",
    "cafes_reviews_df['precrocessed_review'] = cafes_reviews_df['precrocessed_review'].map(remove_stopwords)\n",
    "cafes_reviews_df['precrocessed_review'] = cafes_reviews_df['precrocessed_review'].map(detokenize)\n",
    "cafes_reviews_df['precrocessed_review'] = cafes_reviews_df['precrocessed_review'].map(lemmatize_sentence)\n",
    "\n",
    "# checking the clean data \n",
    "cafes_reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above outputs from the respective categories, it is evident that we have now sucessfully extracted and cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the clean & Preprocessed data in a new CSV file\n",
    "\n",
    "automobile_reviews_df.to_csv('data/clean/automobile_reviews.csv', index = False)\n",
    "fashion_reviews_df.to_csv('data/clean/fashion_reviews.csv', index = False)\n",
    "cafes_reviews_df.to_csv('data/clean/cafes_reviews.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='red'>Section 2:</font> Building a classification Model for each of the Three categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining a global function for calculating and printing the evaluation measures of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list= []\n",
    "train_time_list=[]\n",
    "model_list = []\n",
    "precision_list=[]\n",
    "recall_list=[]\n",
    "error_list=[]\n",
    "fp_list=[]\n",
    "specificity_list=[]\n",
    "f_score_list=[]\n",
    "\n",
    "def reset_lists():\n",
    "    accuracy_list.clear()\n",
    "    train_time_list.clear()\n",
    "    model_list.clear()\n",
    "    precision_list.clear()\n",
    "    recall_list.clear()\n",
    "    error_list.clear()\n",
    "    fp_list.clear()\n",
    "    specificity_list.clear()\n",
    "    f_score_list.clear()\n",
    "\n",
    "# Function to evaluate performance of a model\n",
    "def evaluate_and_show(Y_test,Y_Pred, training_time, model):\n",
    "    confusion_mat=confusion_matrix(Y_test,Y_Pred)\n",
    "    model_list.append(model)\n",
    "    print(\"The Training time for the model: %0.3fs\" % training_time)\n",
    "    train_time_list.append(\" %0.3fs\" % training_time)\n",
    "    \"\"\"This method shows different rates calculated using confusion matrix\"\"\"\n",
    "    total = sum(sum(confusion_mat))\n",
    "\n",
    "    #TN = True NO - accurate negative review \n",
    "    #TY = True Yes - accurate positive review\n",
    "    #FN = False No - inacurate negative review : we have to evaluate how good our classifyer is \n",
    "    #FY = False Yes - inacurate positive review : we have to evaluate how good our classifyer is \n",
    "    TN, FN = confusion_mat[0][0], confusion_mat[1][0]\n",
    "    TY, FY = confusion_mat[1][1], confusion_mat[0][1]\n",
    "    \n",
    "    # Evaluation and CALCULATIONS of parameters using confusion matrix\n",
    "    # Accuracy = Overall, how often is the classifier correct\n",
    "    accuracy = (TN + TY)/ total # true possitive + true negative \n",
    "    accuracy_list.append(accuracy*100) \n",
    "    print(\"Accuracy = {:.2f} %\".format(accuracy*100))\n",
    "    \n",
    "    #Error % = Overall, how often is it wrong\n",
    "    error_rate = 1 - accuracy\n",
    "    error_list.append(error_rate)\n",
    "    print(\"Error % = {:.2f} %\".format(error_rate*100))\n",
    "    \n",
    "    #Precision: When it predicts yes, how often is it correct? \n",
    "    precision = TY/(FY + TY)\n",
    "    precision_list.append(precision*100)\n",
    "    print(\"Precision = {:.2f} %\".format(precision*100))  \n",
    "\n",
    "    #Recall (Sensitivity): When it's actually yes, how often does it predict yes \n",
    "    Recall = TY/(TY + FN)\n",
    "    recall_list.append(Recall*100)\n",
    "    print(\"Recall = {:.2f} %\".format(Recall*100))\n",
    "    \n",
    "    # F1 Score : harmonic mean of Precsison and recall\n",
    "    f_score_list.append(f1_score(Y_test,Y_Pred, pos_label='positive'))\n",
    "    print(\"F1-Score = {:.2f}\".format(f1_score(Y_test,Y_Pred, pos_label='positive')))\n",
    "    \n",
    "    #False Yes Rate: When it's actually no, how often does it predict yes\n",
    "    FY_rate = FY/(TN + FY)\n",
    "    fp_list.append(FY_rate*100)\n",
    "    print(\"False Positive rate = {:.2f} %\".format(FY_rate*100))\n",
    "    \n",
    "    #True No Rate(Specificity): When it's actually no, how often does it predict no\n",
    "    Specificity = TN/(TN + FY)\n",
    "    specificity_list.append(Specificity*100)\n",
    "    print(\"Specificity/True Negative rate = {:.2f} %\".format(Specificity*100))\n",
    "    \n",
    "    # Print the plot for Confusion matrix \n",
    "    fig, ax = plt.subplots(figsize=(7,7))  # figsize is the size of the figure\n",
    "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                    confusion_mat.flatten()]\n",
    "    df_cm = pd.DataFrame(confusion_mat, ['NEG', 'POS'], ['NEG', 'POS'])\n",
    "    labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n",
    "              zip(group_names,group_counts)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    sns.heatmap(df_cm, annot=labels, fmt='')\n",
    "\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.title(\"Confusion Matrix\",fontsize=15)\n",
    "    plt.rcParams.update({'font.size': 13})\n",
    "    plt.show()\n",
    "  \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach: In this section we have three categories of review and we will be building a classification model for each of category. Although the assignment requires us to use any one algorithm, I have built the model using using all the three algorithms namely, Naive Bayes, Logistic Regression and Random Forest in order to see how these models perform on each of the category and then see the best performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category A: Automobile Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_lists()\n",
    "# reading the clean data\n",
    "automobile_reviews_df = pd.read_csv('data/clean/automobile_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the distribuion of the class labels\n",
    "print(automobile_reviews_df['Rating'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the distribution of the classes in this category is slighlty imbalanced as there are more positive class labels in the dataset. Althought the skewness is small we will still use F1-Score along with the model's accuracy as a measure for evaluating the performnace in this case as accuracy alone can yeild a misleading result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: Random Forest Classifier  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the TFIDF for the text review\n",
    "\n",
    "tfidfconverter = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = tfidfconverter.fit_transform(automobile_reviews_df['precrocessed_review']).toarray()\n",
    "\n",
    "# Splitting it into train-test Split (70% for training and remaining 30% for testing)\n",
    "\n",
    "y= automobile_reviews_df['Rating']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=66)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout the notebook for all the models, I have used TFIDF vectorizer in order to convert review text into the corresponding numerical form. There are several approaches of doing this task namely, bag of words (term frequency method), TFIDF ( Term Frequency Inverse document Frequency), Word embedding etc. I have chosen TFIDF because it is effectiveness and simiplicity. The bag of words approach yeilds good results when we converting text to numbers. However, it suffers from one major drackback. The Bag of words approach assigns a score to the words based on their occurrence in a particular document. It ignores the fact that the same word might be having a high frequency of occurrence in other documents as well therby conveying less information. TFIDF on the other hand is better at this because it multiplys the term frequency of a word by the inverse document frequency. The TF stands for \"Term Frequency\" while IDF stands for \"Inverse Document Frequency\". Therfore, the TFIDF score for a specific word in a particular document is higher if and only if the frequency of occurrence of that word is higher in that specific document but lower in all the other documents.\n",
    "\n",
    "Furthermore, I have restricted the maximum number of features to 1500 only because of two reasons:\n",
    "1. A review can contain hundreds of words and not all of the are significant. We dont want to calculate the TFIDF score for each and every word\n",
    "2. We have to build a model that can be tested on a completely differnt dataset in section 3 therfore, if we do not specify the limit at first place while building the model we will get an error because the number of features in the trained model will not match the number of features in the test dataset.\n",
    "\n",
    "\n",
    "Finally, In order to get the accurate estimate for the model's performance, we need to adopt simple hold out strategy where in we divide our data into training and testing sets. We train our model on the train set adn will test the trained model on the test set. To do so, we will use the train_test_split utility from the sklearn.model_selection library. In this notebook, all models have been trained on 70% of the dataset termed as Trainig set adn 30% of test data.\n",
    "\n",
    "Note: I have used pickle to save the trained model for future use as it becomes very convinient to just load the model whenever needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forrests\n",
    "print(\"Training Random forest Classifier.......\")\n",
    "classifier = RandomForestClassifier(n_estimators=3000, random_state=42)\n",
    "t = time()\n",
    "classifier.fit(X_train, y_train)\n",
    "training_time = time() - t\n",
    "print(\"Training Complete !\")\n",
    "y_pred = classifier.predict(X_test)\n",
    "#saving the model for use in task 3 \n",
    "pickle.dump(classifier, open('data/models/rfc_automobile.sav', 'wb'))\n",
    "# Genrating the COnfusion Matrix \n",
    "conf_mat=confusion_matrix(y_test,y_pred)\n",
    "# for random forrests\n",
    "evaluate_and_show(y_test, y_pred, training_time, 'Random Forests')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the Random Forest classification performs quite well in the classification task with the overall accuracy of 86.33% and F1-Score of 0.89. The F1-score is the harmonic mean of precison and recall. Another reason we focus on F1-Score is because here both the positive and the negative classes are significant and therefore, we want to find an optimal blend of precision and recall we can combine the two metrics using what is called the F1 score.\n",
    "The train train of the model even for a smaller dataset is quite high for Random forest, this is because the random forest uses ensemble learning and henceforth a large number of decison trees are used for the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2 - Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naivebayes\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "t = time()\n",
    "naive_bayes_classifier.fit(X_train, y_train)\n",
    "pickle.dump(naive_bayes_classifier, open('data/models/nbc_automobile.sav', 'wb'))\n",
    "training_time = time() - t\n",
    "y_pred = naive_bayes_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_show(y_test, y_pred, training_time, 'Naive Bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above results, we can see that Naive bayes performed very well in the classification task. The accuracy of the model is 91.67 along with F1-Score of 0.93 which is very good. Moreover, given the low training time of the model, the overall performance of this model is one best that we can achieve.\n",
    "The High count of True positive and True negatives in the confusion matrix above further confirm the fact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3 - Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# define model\n",
    "modelLR = LogisticRegression(solver='lbfgs')\n",
    "# fit model\n",
    "t = time()\n",
    "modelLR.fit(X_train, y_train)\n",
    "training_time = time() - t\n",
    "# save the model for future use in section 3\n",
    "pickle.dump(modelLR, open('data/models/lr_automobile.sav', 'wb'))\n",
    "# make predictions\n",
    "y_pred = modelLR.predict(X_test)\n",
    "# evaluate predictions\n",
    "evaluate_and_show(y_test, y_pred, training_time, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of Logistic regression is almost comparable to that of Naive Bayes. The model achieves a very high accuracy and F1-score of 90.33 and 0.92 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_compare():\n",
    "    performance_df = pd.DataFrame()\n",
    "    performance_df['Model']=model_list\n",
    "    performance_df['Accuracy']=accuracy_list\n",
    "    performance_df['Precision']=precision_list\n",
    "    performance_df['Recall']=recall_list\n",
    "    performance_df['F1-Score']=f_score_list\n",
    "    performance_df['Specificity']=specificity_list\n",
    "    performance_df['False Positive Rate']=fp_list\n",
    "    performance_df['Train Time']= train_time_list\n",
    "\n",
    "    th_props = [\n",
    "        (\"font-size\", \"120%\"),\n",
    "        ('text-align', 'center'),\n",
    "        ('font-weight', 'bold'),\n",
    "        ('color', '#6d6d6d'),\n",
    "        ('background-color', '#f7f7f9')\n",
    "        ]\n",
    "\n",
    "        # Set CSS properties for td elements in dataframe\n",
    "    td_props = [\n",
    "        ('font-size', '11px')\n",
    "         ]\n",
    "\n",
    "        # Set table styles\n",
    "    styles = [\n",
    "        dict(selector=\"th\", props=th_props),\n",
    "        dict(selector=\"td\", props=td_props)\n",
    "        ]\n",
    "    \n",
    "    if(len(set(train_time_list))==1):\n",
    "        return (performance_df.style\n",
    "                .highlight_min(subset=['False Positive Rate'], color='lightgreen')\n",
    "                .set_caption('SUMMARY OF THE PERFORMANCE OF ALL THE TESTED MODELS IN SECTION 3')\n",
    "                .set_table_styles(styles)).hide_index()\n",
    "    \n",
    "    else: \n",
    "        return(performance_df.style\n",
    "            .highlight_max(subset=['Accuracy','F1-Score'], color='lightgreen')\n",
    "            .highlight_min(subset=['Accuracy','F1-Score'], color='pink')\n",
    "            .set_caption('The Best Performing Model in Green and Worst Performing in Red')\n",
    "            .set_table_styles(styles)).hide_index()\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table provides a comparasion between all the three alogithms when applied on the automobile category. It is evedent from the table for automobile category the best performing model is naive bayes while the worst performing model is Random Forests. Since we are primarily focussing on just the two measures namely, accuracy and F1-score I have highlighted only those two fields. \n",
    "The end results ar not quite what I was expecting. I expected Random forests to perform much better since it employs the ensemble learning technnique which genrally speak yeilds better results that the individual classification algorithms but that was not the case over here. In fact, Random forest Classifier performed the worst of the three. It would be interesting to see how it fairs in the other caategories in the subsequent sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category B: Fashion Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_lists()\n",
    "fashion_reviews_df = pd.read_csv('data/clean/fashion_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the distribuion of the class labels\n",
    "print(fashion_reviews_df['Rating'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the previous category; Automobile, the class labels in this category too are sckewed in the favor of positive class and therfore, we will continue with our strategy of using F1-score along with the accuracy score to evaluate the performance of the model. At prima-facie we see that the businesses in the automoobile and fashion category are doing quite well as both seem to have more positive reviews than the negative ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1 - Random Forests Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the TFIDF for the text review\n",
    "\n",
    "tfidfconverter = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = tfidfconverter.fit_transform(fashion_reviews_df['precrocessed_review']).toarray()\n",
    "\n",
    "# Splitting it into train-test Split (70% for training and remaining 30% for testing)\n",
    "\n",
    "y= fashion_reviews_df['Rating']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forrests\n",
    "print(\"Training Random forest Classifier.......\")\n",
    "classifier = RandomForestClassifier(n_estimators=3000, random_state=42)\n",
    "t = time()\n",
    "classifier.fit(X_train, y_train)\n",
    "training_time = time() - t\n",
    "print(\"Training Complete !\")\n",
    "y_pred = classifier.predict(X_test)\n",
    "#saving the model for use in task 3 \n",
    "pickle.dump(classifier, open('data/models/rfc_fashion.sav', 'wb'))\n",
    "# for random forrests printing the resuts and performance\n",
    "evaluate_and_show(y_test, y_pred, training_time, 'Random Forests')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the perfromace of Random Forest reamins consistent with the overall accuracy of 85.67% and the F1-score of 0.89 on the 30% of the test data from the fashion category. The taining time hoverever has increased in this category as compared to the previous  category.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2 - Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naivebayes\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "t = time()\n",
    "naive_bayes_classifier.fit(X_train, y_train)\n",
    "pickle.dump(naive_bayes_classifier, open('data/models/nbc_fashion.sav', 'wb'))\n",
    "training_time = time() - t\n",
    "y_pred = naive_bayes_classifier.predict(X_test)\n",
    "evaluate_and_show(y_test, y_pred, training_time, 'Naive Bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performnace of Naive Bayes for Fashion category is not as good as that for the Automobile category. Naive bayes performs just as well as the Random Forest over here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3 - Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# define model\n",
    "modelLR = LogisticRegression(solver='lbfgs')\n",
    "# fit model\n",
    "t = time()\n",
    "modelLR.fit(X_train, y_train)\n",
    "training_time = time() - t\n",
    "# save the model for a use later on \n",
    "pickle.dump(modelLR, open('data/models/lr_fashion.sav', 'wb'))\n",
    "# make predictions\n",
    "y_pred = modelLR.predict(X_test)\n",
    "# evaluate predictions\n",
    "evaluate_and_show(y_test, y_pred, training_time, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Automobile category, we can see that all the three models yeild more or less the same results both in terms of the accuracy as well as the F1 score. There is nothing much to differentiate their performance, if we had to pick one model, it would be Logistic Regression. It appears that the Fashion category contains some tricky reviews that has lead to a drop in performance of all of the three models compared to the Automobile category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category C: Cafe Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_lists()\n",
    "cafe_reviews_df = pd.read_csv('data/clean/cafes_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the distribuion of the class labels\n",
    "print(cafe_reviews_df['Rating'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data in this category is skewed to a large degree towards the postive class. The ratio seems to be perticularly high for the cafe category where there are 1462 positive reviews as opposed to just 538 negative reviews. In such cases there is a risk that the classifier only learns to classify the positive reviews and labels everything as positive. Therefore, we will be focssuing on F1 score here and along with it we will also calculate the Area Under the Curve (AUC) of the  Reciever- operator characteristic with cross validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1 - Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the TFIDF for the text review\n",
    "\n",
    "tfidfconverter = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = tfidfconverter.fit_transform(cafe_reviews_df['precrocessed_review']).toarray()\n",
    "\n",
    "# Splitting it into train-test Split (70% for training and remaining 30% for testing)\n",
    "\n",
    "y= cafe_reviews_df['Rating']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forrests\n",
    "print(\"Training Random forest Classifier.......\")\n",
    "classifier = RandomForestClassifier(n_estimators=3000, random_state=42)\n",
    "t = time()\n",
    "classifier.fit(X_train, y_train)\n",
    "training_time = time() - t\n",
    "print(\"Training Complete!\")\n",
    "y_pred = classifier.predict(X_test)\n",
    "#saving the model for use in task 3 \n",
    "pickle.dump(classifier, open('data/models/rfc_cafe.sav', 'wb'))\n",
    "\n",
    "# for random forrests printing the resuts and performance\n",
    "evaluate_and_show(y_test, y_pred, training_time, 'Random Forests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(modelName, name):\n",
    "    # roc curve and auc\n",
    "    from sklearn.datasets import make_classification\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from matplotlib import pyplot\n",
    "    # generate a no skill prediction (majority class)\n",
    "    ns_probs = [0 for _ in range(len(y_test))]\n",
    "\n",
    "    # predict probabilities\n",
    "    lr_probs = modelName.predict_proba(X_test)\n",
    "    # keep probabilities for the positive outcome only\n",
    "    lr_probs = lr_probs[:, 1]\n",
    "    # calculate scores\n",
    "    ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "    lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "    # summarize scores\n",
    "    print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "    print( name, ': ROC AUC=%.3f' % (lr_auc))\n",
    "   # print('name : ROC AUC=%.3f' % (lr_auc))\n",
    "    # calculate roc curves\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs, pos_label='positive')\n",
    "    lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs, pos_label='positive')\n",
    "    # plot the roc curve for the model\n",
    "    pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "    pyplot.plot(lr_fpr, lr_tpr, marker='.', label= name)\n",
    "    # axis labels\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    # show the legend\n",
    "    pyplot.legend()\n",
    "    # show the plot\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(classifier, 'Random Forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the results that the model performs very well. The accuracy and the F1-sccore of 84% and 0.90 respectively suggests that the model is accurate enough to predict the class labels. The distribution in the Confusion matrix suggests that the model has learned the features of both the classes the final evidence that the model is performing very well even on seemingly skewed dataset is from the ROC-AUC curve shown above. An excellent model which is hypothetical and does not exist, has AUC near to the 1 which means it has good measure of separability. A model that did not learn anything and is randomly assigning class labels would have the score of 0.5 and is shown by the blue line in the graph above. It means model has no class separation capacity whatsoever. In our case, the model has a score of 0.884 and at no does the model overlap with the no-skill model therefore we can confirm that the model is performing very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2 - Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naivebayes\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "t = time()\n",
    "naive_bayes_classifier.fit(X_train, y_train)\n",
    "pickle.dump(naive_bayes_classifier, open('data/models/nbc_cafe.sav', 'wb'))\n",
    "training_time = time() - t\n",
    "y_pred = naive_bayes_classifier.predict(X_test)\n",
    "evaluate_and_show(y_test, y_pred, training_time, 'Naive Bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(naive_bayes_classifier, 'Naive Bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Naive bayes again does a good job here, the Accuracy and F1-score both are very good. The large number of false positives in the Confusion matix shows the underlying skewness in the data set. However the ROC-AUC curve shows that the classifier is able to distinguish between the class labels as in case with the Random Forest therfore, overall the performance is satisfactory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3 - Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# define model\n",
    "modelLR = LogisticRegression(solver='lbfgs')\n",
    "# fit model\n",
    "t = time()\n",
    "modelLR.fit(X_train, y_train)\n",
    "training_time = time() - t\n",
    "# save the model for a use later on \n",
    "pickle.dump(modelLR, open('data/models/lr_cafe.sav', 'wb'))\n",
    "# make predictions\n",
    "y_pred = modelLR.predict(X_test)\n",
    "# evaluate predictions\n",
    "evaluate_and_show(y_test, y_pred, training_time, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(modelLR, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression performs exceedingly well, the accuracy and the F1-scores are both very good and most importantly the model has least false positives amongst all the three models. Furthermore, the model has an high ROC AUC score of 0.913 that clearly indicates its ability to distinguish between the two class labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final category too we see that Logistic Regression outperformed the Random Forest and barring the first category, where it was defeated by Naive bayes but only by a small margin, Logistic regresion has been the best overall classifier for classification of the reviews.\n",
    "A point worth nooticing over her is that in each category, logistic regression outperformed random Forest which wasn't something I was expecting. Random Forest, employs ensemble learning as they use multiple decision trees( ensemble of decision trees) and generally speaking ensembles tend to perform better than individual classifiers. However, in this case where we were dealing with binary classification and given that the dataset was relatively small justifies the results to some extent. Logistic regression does a better job of classification in such scenarios and takes only a fraction of the total training time that a random forest would take. Now, once we have the best performing model from each category, I wanted to test this best performing model trained on one category to predict the class labels of other categories. Which brings us to the next and final section of the notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Section 3:</font> Cross Category model testing: Train on one Category and test on other two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.a: Train on Automobile and test on Fashion and Cafe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performing model in the Automobile category was Naive bayes. Therfore we will be begining the third section with Loading our saved model which was already trained earlier on the Automobile category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_lists()\n",
    "\n",
    "# storing the TFIDF values of all of the categories\n",
    "\n",
    "#1.Automobile category dataset\n",
    "tfidfconverter = TfidfVectorizer(max_features=1500, ngram_range=(1, 2), min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X_automobile = tfidfconverter.fit_transform(automobile_reviews_df['precrocessed_review']).toarray()\n",
    "Y_automobile = automobile_reviews_df['Rating']\n",
    "\n",
    "#2.Fashion category dataset\n",
    "tfidfconverter = TfidfVectorizer(max_features=1500, ngram_range=(1, 2), min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X_fashion = tfidfconverter.fit_transform(fashion_reviews_df['precrocessed_review']).toarray()\n",
    "Y_fashion= fashion_reviews_df['Rating']\n",
    "\n",
    "#3.Cafe category dataset\n",
    "tfidfconverter = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X_cafe = tfidfconverter.fit_transform(cafe_reviews_df['precrocessed_review']).toarray()\n",
    "Y_cafe= cafe_reviews_df['Rating']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='red'>Case 1:</font> \n",
    "###### Train: Automobile Dataset\n",
    "###### Test: Fashion Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the model for Naive bayes\n",
    "load_automoible_trained_nbc = pickle.load(open('data/models/nbc_automobile.sav', 'rb'))\n",
    "\n",
    "# after loading the model we use that for prediction\n",
    "y_pred = load_automoible_trained_nbc.predict(X_fashion)\n",
    "evaluate_and_show(Y_fashion, y_pred, 0, 'Automibile trained NB tested on Fashion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes classifier trained on Automobile category performs very poorly on the fashion category dataset. The model has an error rate of 48.40% which is very high. Furthermore, we can also see that the model is falsely predicting a lot of positive reviews as negative and this can be seen on the Confusion matrix with 657 false negatives.\n",
    "We can clearly see that while Niave Bayes performed exceedingly well when trained on a subset of Automobile Categeory and tested on a different subset within the same category. The model was not able to pick up import feaatures from that category which could be genralized and hence performed poorly when tested on a different category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the performance of the best model is poor I decided to test the other two models from the same category to see how they fair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the model \n",
    "load_automoible_trained_rfc = pickle.load(open('data/models/rfc_automobile.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forrests\n",
    "y_pred = load_automoible_trained_rfc.predict(X_fashion)\n",
    "\n",
    "# for random forrests printing the resuts and performance\n",
    "evaluate_and_show(Y_fashion,y_pred, 0, 'Automibile trained RF tested on Fashion ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Random Forest classifier the performence improves bu only slightly and that too at a cost. There's just a 4% increase in the overall accuracy of the model comapred to Naive Bayes. The confusion Matrix and the evaluation measures show that the model has very high false positive rate which means that the model is falsely classifying the negative reviews as positive. This is not a good sign for a model. With a false positive rate of almost 71 % percent we are missing out on the important negative reviews of the cutomers about the business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the model for Logistic Regression\n",
    "load_automoible_trained_lr = pickle.load(open('data/models/lr_automobile.sav', 'rb'))\n",
    "y_pred = load_automoible_trained_lr.predict(X_fashion)\n",
    "\n",
    "# evaluate predictions\n",
    "evaluate_and_show(Y_fashion,y_pred, 0, 'Automibile trained LR tested on Fashion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of Logistic Regression is better compared to the other two models in terms of the accuracy and F1-score but all the three models produce very number of false positives as seen in the confusion matrix. What this means is that the model is falsely classifying the negative reviews as positive this defeats the purpose of having the reivews and classifying them. Businesses would want to classify the large set of reviews into positive and negative and then focus on the negative reviews inorder to improve itself. If the classifier has a false positive rate of 75% like in ths case then it would fail to pick up on the crucial reviews. One final way to test the model is to see how it performs in the RoC AUC test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='red'>Case 2:</font> \n",
    "###### Train: Automobile Dataset\n",
    "###### Test: Cafe Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the model Logistic\n",
    "load_automoible_trained_lr = pickle.load(open('data/models/lr_automobile.sav', 'rb'))\n",
    "y_pred = load_automoible_trained_lr.predict(X_cafe)\n",
    "# evaluate predictions\n",
    "evaluate_and_show(Y_cafe,y_pred, 0, 'Automibile trained LR tested on Cafe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the automobile trained logistic Regression classifier was tested on the cafe category, the model initially seemed to have performed very well because of the high accuracy but again the model has high accuracy but that accuracy is of no use because of the very high false positive rate. As described in the previous section, the model is not effective enough to be used because of false positive rate. One reason could be that all of the three categories have nothing in common by common I mean that the words used in the autmobile buisiness reviews will be completely different than those used in the review of cafe the later will involve words that deal with food and hospitality industry. While the fashion category would involve words that are related to the fashion and beauty industry. It is cear that none of the models have learned the generic meaning of the terms as a result of which all perform very well in their own category but fail when dealing with reviews from other category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the next two categories, I have tested only the best performing model from that category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.b: Train on Fashion and test on Automobile and Cafe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='red'>Case 1:</font> \n",
    "###### Train: Fashion Dataset\n",
    "###### Test: Automobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#loading the model of fashion trained dataset\n",
    "load_fashion_trained_lr = pickle.load(open('data/models/lr_fashion.sav', 'rb'))\n",
    "y_pred = load_fashion_trained_lr.predict(X_automobile)\n",
    "# evaluate predictions\n",
    "evaluate_and_show(Y_automobile,y_pred, 0, 'Fashion trained LR tested on Automobile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model here has an accuracy of about 58% and F1-Score of 0.71 which are poor to the say the least. furthermore, the model again has very high false positive rate of 77.66% which means a lot of the negative reviews have beeb predicted as positives. There are only 176 true negatives which is too less a number for a model to be effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='red'>Case 2:</font> \n",
    "###### Train: Fashion Dataset\n",
    "###### Test: Cafe Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = load_fashion_trained_lr.predict(X_cafe)\n",
    "# evaluate predictions\n",
    "evaluate_and_show(Y_cafe,y_pred, 0, 'Fashion trained LR tested on Cafe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test on cafe dataset seems to yeld a considerably high accuracy of approxiamtley 67 % but model does very well as it categories majority of the reviews as positive and therfore yielding a high accuracy as some of them will be correct. In the process of doing so the model is misclassifying negative reviews as positive.Like all other models we tested so far. This model too is no different from others. Such a model has misleaading accuraacy and will not be effective enough in serving its purpose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.c: Train on Cafe and test on Automobile and Fashion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='red'>Case 1:</font> \n",
    "###### Train: Cafe Dataset\n",
    "###### Test: Automobile Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the model\n",
    "load_cafe_trained_lr = pickle.load(open('data/models/lr_cafe.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = load_cafe_trained_lr.predict(X_automobile)\n",
    "# evaluate predictions\n",
    "evaluate_and_show(Y_automobile,y_pred, 0, 'Cafe trained LR tested on Automobile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A look at the Confusion matrix shows how bad the model actually performs. There are just 8 true negatives which in it self shows us that the model is not suitable to be deployed. If we were to randomly classify reviews into positive and negatives, we would perform better than this. Thr false positive rate of 98.98 % is simply not acceptable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='red'>Case 2:</font> \n",
    "###### Train: Cafe Dataset\n",
    "###### Test: Fashion Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = load_cafe_trained_lr.predict(X_fashion)\n",
    "# evaluate predictions\n",
    "evaluate_and_show(Y_fashion,y_pred, 0, 'Cafe trained LR tested on fashion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model too suffers from the same problem like its predecessor. A vey low true negative rate and a very high false positive rate indicates that the model simply labels majority of the reviews as positve as opposed to learning the features for each class and then calssifying them. The  model model therfore is again does not genralize well and hence cannot be used to claasify reviews from the other category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='red'>Section 3 Conclusion: </font> \n",
    "The above table shows a summary of all the performance metric of each of the model tested in section 3. We can clearly see that all the models suffer from the same problem:\n",
    "They have very high false positive rate as is the case with all the models in the above table except 'Automobile Trained Naive Bayes on fashion Category. For any Business both positive and negative reviews are important. Positive reviews help the business understand what areas they are dooing good but earning perfect positive reviews isn’t critical to the businesses its the negative reviews that can help them grow hence negative reviews carry way more significane than the positive ones. Negative reviews provide a more realistic look of the business and actually provide more trust than having all 5 Star positive reviews. The reason why negative ones are looked at in the first place is to find out why the customers  were dissatisfied with the product or the service being offered. Hence we need to have a classifier that is able to detect both the kinds of review and what we surely do not want is a classifier that labels all the reviews as postive ones. That means we do not want a high False positive rate. If we have models with high false positive rates then it means that the model will misclassify negative reviews as positives and defeat the entire purpose of having those reviews at first place. \n",
    "\n",
    "We see that all the above models did very well when we trained them and tested them on the reviews from the same category but in over here when we tested them on reviews from other category all of them except one produced significantly high false positive rates. The one that did not had an accuracy of 51.6% which is as good as randomly assigning class labels to the reviews. One major reason for this could be the lack of similarity in the three selected categories. All three of them; automobile, fashion and cafe are very differnet from one another. They are three distinct industries and henceforth the words used in the reviews would also be very different therefore the model trained in one category does well in the same category but performs poorly on data from the other two categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: \n",
    "\n",
    "In section 1 of the notebook, we performed scraping of the reviews from three different categories of the businesses and stored them for the classification task. I decided to choose the automobile, Fashion, and Cafes because these three categories did not have anything in common as mentioned in the previous section. I wanted to check how the models will perform on categories that were the most dissimilar.\n",
    "\n",
    "In section 2, I trained and tested all three algorithms ( Random Forests, Naïve Bayes, Logistic Regression) on each of the three categories to examine the best performing model for each category. Generally speaking, for each of the categories, logistic regression outperformed random Forest. This was not something which I was expecting. Random Forest, employs ensemble learning as they use multiple decision trees( ensemble of decision trees), and generally speaking ensembles perform better than the individual classifiers . However, in this case given the relatively small size of the dataset and given the fact it involves only two classes (binary classification) the results can be justified. Logistic regression does a better job of classification in such scenarios and it is evident here. Now, once I had the best performing model from each category, I wanted to test this best performing model trained on one category to predict the class labels of other categories.\n",
    "\n",
    "The best performing model in the case of the automobile category turned out to be Naïve Bayes though the difference in performance for Naïve Bayes and Logistic regression was not much, nevertheless Naïve Bayes yielded best results for the automobile category. Following which I tested this model which was trained on Automobile on the other two categories I got a dismal accuracy of around 45% on both categories i.e. cafe and fashion this suggests that while naive Bayes performs exceedingly well on the test train split from the same category, it does not do well while predicting the labels of other two categories. This means it does not generalize across the categories. Therefore, I decided to test the other two models from the automobile category in fashion to see how they fare. It was found that the other two models did not perform well either. An important evaluation measure in this category is the False positive rate and all of the built models performed poorly in this evaluation.\n",
    "\n",
    "Thus, we can conclude that logistic regression is best choice of algorithm when the data is from the same category and we test it on the same category. I had selected the most dissimilar categories in order to see how well the models generalize by picking up positive and negative features from the dataset and then using the same features to predict the labels for another category but after sufficient testing, it can be concluded that the model’s performance is dependent  on the similarity on the words between the train and test data set.\n",
    "\n",
    "In future, we could add bi-gram and tri-gram analysis and then select the most informative features from it. Along with that we could add sentiment analysis and count the number of positive and negative words in a review by comparing it to one of the available sentiment word list and then run classifer on it. I belive such a technique would allow the models to generalize better.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
